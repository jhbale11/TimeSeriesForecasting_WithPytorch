{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-Layer LSTM\n",
    "\n",
    "- 저장한 수익률 데이터 불러오기\n",
    "- 모델 생성 및 가중치 초기화\n",
    "- 모델 학습\n",
    "- 모델 평가\n",
    "- 예측 진행 및 결과 확인\n",
    "\n",
    "## Overall Process\n",
    "![](/home/jhbale11/TimeSeriesForecasting_WithPytorch/img/Process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhbale11/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta \n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from itertools import cycle\n",
    "import datetime as dt\n",
    "\n",
    "# matplotlib 설정\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 재생산을 위해 SEED를 고정하겠습니다.\n",
    "SEED = 1345\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('./data/samsung_df.csv')\n",
    "date_index = y['Date']\n",
    "dates = date_index[0:1248]\n",
    "dates_list = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in dates]\n",
    "y.index = pd.to_datetime(y.Date)\n",
    "y = y.drop(['Date'], axis=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(y)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "train_data_normalized = scaler.fit_transform(data.reshape(-1,1))\n",
    "train_data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 함수는 28일로 이루어져 있는 sequences와 하루의 label을 생성합니다.\n",
    "def sliding_windows(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 21\n",
    "x, y = sliding_windows(train_data_normalized, seq_length)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(y) * 0.8)\n",
    "test_size = len(y) - train_size\n",
    "\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
    "\n",
    "testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n",
    "\n",
    "print(\"train shape is:\",trainX.size())\n",
    "print(\"train label shape is:\",trainY.size())\n",
    "print(\"test shape is:\",testX.size())\n",
    "print(\"test label shape is:\",testY.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer LSTM\n",
    "![](/home/jhbale11/TimeSeriesForecasting_WithPytorch/img/LSTM-Multi.png)\n",
    "\n",
    "- `num_classes` : \n",
    "- `num_layer` : LSTM 층을 얼마나 쌓을지 결정하는 변수입니다.\n",
    "- `input_size` : input의 feature dimension을 넣어야 합니다. `time_step`을 넣어주는 것이 아니라 input feature dimension을 넣어주어야 합니다. 우리의 경우 21개의 샘플로 이루어진 하나의 input을 넣어주는 것이기 때문에 `1`으로 하겠습니다.\n",
    "- `hidden_size` : 내부에서 어떤 Feature Dimension으로 바꿔주고 싶은지에 대한 변수입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM2, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = 1\n",
    "        #self.seq_length = seq_length\n",
    "\n",
    "        self.LSTM2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,batch_first=True,dropout = 0.25)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        \n",
    "        c_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "       \n",
    "        _, (hn, cn) = self.LSTM2(x, (h_1, c_1))\n",
    "     \n",
    "        #print(\"hidden state shpe is:\",hn.size())\n",
    "        y = hn.view(-1, self.hidden_size)\n",
    "        \n",
    "        final_state = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1]\n",
    "        #print(\"final state shape is:\",final_state.shape)\n",
    "        out = self.fc(final_state)\n",
    "        #out = self.dropout(out)\n",
    "        #print(out.size())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 1\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM2(num_classes, input_size, hidden_size, num_layers)\n",
    "lstm.to(device)\n",
    "lstm.apply(init_weights)\n",
    "\n",
    "criterion = torch.nn.MSELoss().to(device)    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=100, factor =0.5 ,min_lr=1e-7, eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in progress_bar(range(num_epochs)): \n",
    "    lstm.train()\n",
    "    outputs = lstm(trainX.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    torch.nn.utils.clip_grad_norm_(lstm.parameters(), 1)\n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, trainY.to(device))\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    scheduler.step(loss)\n",
    "    optimizer.step()\n",
    "    lstm.eval()\n",
    "    valid = lstm(testX.to(device))\n",
    "    vall_loss = criterion(valid, testY.to(device))\n",
    "    scheduler.step(vall_loss)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "      print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f \" %(epoch, loss.cpu().item(),vall_loss.cpu().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Prediction###############\n",
    "lstm.eval()\n",
    "train_predict = lstm(dataX.to(device))\n",
    "data_predict = train_predict.cpu().data.numpy()\n",
    "dataY_plot = dataY.data.numpy()\n",
    "\n",
    "data_predict = scaler.inverse_transform(data_predict)\n",
    "dataY_plot = scaler.inverse_transform(dataY_plot)\n",
    "\n",
    "## Add dates\n",
    "df_predict = pd.DataFrame(data_predict)\n",
    "df_predict = df_predict.set_index([dates_list[:-21]])\n",
    "df_labels = pd.DataFrame(dataY_plot)\n",
    "df_labels = df_labels.set_index([dates_list[:-21]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "figure(num=None, figsize=(19, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.axvline(x=dates_list[train_size], c='r')\n",
    "plt.plot( df_labels[0])\n",
    "plt.plot(df_predict[0])\n",
    "plt.legend(['Prediction','Time Series'],fontsize = 21)\n",
    "plt.suptitle('Time-Series Prediction Entire Set',fontsize = 23)\n",
    "plt.xticks(fontsize=21 )\n",
    "plt.yticks(fontsize=21 )\n",
    "plt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\n",
    "plt.xlabel(xlabel = 'Date',fontsize = 21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Plot the test set ##########################\n",
    "figure(num=None, figsize=(23, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(df_labels.iloc[-testX.size()[0]:][0])\n",
    "plt.plot(df_predict.iloc[-testX.size()[0]:][0])\n",
    "plt.legend(['Prediction','Time Series'],fontsize = 21)\n",
    "plt.suptitle('Time-Series Prediction Test',fontsize = 23)\n",
    "plt.xticks(fontsize=21 )\n",
    "plt.yticks(fontsize=21 )\n",
    "plt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\n",
    "plt.xlabel(xlabel = 'Date',fontsize = 21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Calculate RMSE ##################\n",
    "np.sqrt(((dataY_plot[-testX.size()[0]:] - data_predict[-testX.size()[0]:] ) ** 2).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2997bed8b8dc9e6c21460fa4ad9bada3279c9329f989c1464fa47985ad0f56a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
